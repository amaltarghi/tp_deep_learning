{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : MiniNN Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 décembre 2016  \n",
    "Adapté du TP de Gaétan Marceau-Caron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Au cours du TP1, nous avons étudié le modèle _Softmax_ (aussi connu sous le nom de MaxEnt) pour traiter le problème de classification probabiliste.\n",
    "Le but était de présenter deux étapes importantes de l'entraînement: la {\\it forward propagation} et la mise à jour des paramètres.\n",
    "Le TP2 reprend le modèle Softmax dans un cadre plus général, celui des réseaux de neurones avec couches cachées.\n",
    "\n",
    "Dans ce cadre, on peut considérer le modèle Softmax comme un \"module\" qui prend en entrée des \"features\", e.g. les pixels d'une image, et qui donne en sortie une loi de probabilité sur les étiquettes.\n",
    "D'un point de vue computationnel, un réseau de neurones est composé de plusieurs modules, transformant simplement les features d'un espace à un autre en fonction des valeurs courantes des paramètres.\n",
    "Ainsi, le but de l'entraînement est d'apprendre les transformations pertinentes, i.e., en modifiant les paramètres, qui permettront de réaliser la tâche associée au module de sortie. \n",
    "En augmentant le nombre de modules (mais aussi de fonctions non-linéaires), on augmente ainsi la complexité du modèle.\n",
    "La thèse du {\\it Deep Learning} nous dit que les modules près des données d'entrée doivent apprennent des features de bas niveau, e.g., filtre de Gabor pour l'image, alors que les modules près de la sortie apprennent des features de haut niveau, e.g., la probabilité qu'il y ait un chat dans l'image.\n",
    "A priori, cette hiéarchie des features n'est pas imposée par le programmeur, mais apparaît naturellement lors de l'entraînement avec l'algorithme de backpropagation .\n",
    "\n",
    "Le but du TP2 est de programmer les trois étapes essentielles à l'entraînement d'un réseau de neurones: la forward propagation, la backpropagation et la mise à jour des paramètres.\n",
    "Ensuite, vous pouvez créer un test permettant de vérifier l'implémentation: le test des différences finies.\n",
    "Finalement, vous pourrez comparer les performances de votre réseau de neurones avec celles de votre modèle Softmax de la semaine dernière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs : \n",
    "    1. Télécharger le TP2\n",
    "    2. Implementer la fonction forward (nn ops.py:72)\n",
    "    3. Implementer la fonction sigmoid et sa d ́eriv ́ee (nn ops.py:149)\n",
    "    4. Implementer la fonction backward (nn ops.py:93)\n",
    "    5. Implementer la fonction update (nn ops.py:123)\n",
    "    6. Entrainer miniNN et obtenir une accuracy meilleur que le modèle softmax (de 0.92)\n",
    "\n",
    "\n",
    "Et optionellemnt : \n",
    "    7. Implementer le test des diff ́erences finies (fd test.py)\n",
    "    \n",
    "La différence finie est une approximation de la dérivée partielle:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l(w)}{\\partial w_{i}} \\approx \\frac{l(w + \\epsilon e_{i}) - l(w - \\epsilon e_{i})}{2 \\epsilon}\n",
    "\\end{equation}\n",
    "où $l$ est une fonction à plusieurs variables avec ses dérivées partielles définies, $w$ est un vecteur, $w_{i}$ est sa $i$ème composante, $\\epsilon$ est la longeur du pas et $e_{i}$ est le $i$ème vecteur de la base canonique de l'espace euclidien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrable\n",
    "\n",
    "- __Date du livrable__: Le 16 décembre 2015  \n",
    "- __Format du livrable__ un fichier compressé nommé \"DL\\_tp2\\_prénom\\_nom.zip\" contenant le code et le résumé   \n",
    "- __Dépôt__ à l'adresse _{thomas.schmitt@inria.fr}_ avec comme objet du message \"DL\\_tp2\\_prénom\\_nom\".  \n",
    "- __Description__  \n",
    "Le livrable associé au TP2 doit contenir le code de MiniNN complété et accompagné d'un résumé de une à deux pages.\n",
    "Le code doit s'exécuter avec la commande _python miniNN.py_ et afficher l'évolution de l'apprentissage (sortie par défaut du programme).  \n",
    "( Ou bien être sous la forme d'un notebook. )\n",
    "\n",
    "Le résumé doit être succinct et se focaliser uniquement sur les points essentiels reliés à l'entraînement des réseaux de neurones.\n",
    "Ce document doit décrire les difficultés que vous avez rencontrées et, dans le cas échéant, les solutions utilisées pour les résoudre.\n",
    "Vous pouvez aussi y décrire vos questions ouvertes et proposer une expérience sur MNIST afin d'y répondre.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone de Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and minibatch (as in miniNN.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Pour ne pas reload le kernel quand un fichier .py change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math, time, sys\n",
    "import dataset_loader\n",
    "from nn_ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the arguments \n",
    "args = parseArgs_ipython(arch = [800,800], act_func = \"reLU\", batch_size = 500, eta = 0.1, n_epoch = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fix the seed for the random generator\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "#############################\n",
    "### Dataset Handling\n",
    "#############################\n",
    "\n",
    "### Load the dataset\n",
    "train_set, valid_set, test_set = dataset_loader.load_mnist()\n",
    "\n",
    "### Define the dataset variables\n",
    "n_training = train_set[0].shape[0]\n",
    "n_feature = train_set[0].shape[1]\n",
    "n_label = np.max(train_set[1])+1\n",
    "\n",
    "#############################\n",
    "### Neural Network parameters\n",
    "#############################\n",
    "\n",
    "### Activation function\n",
    "act_func_name = args.act_func\n",
    "\n",
    "### Network Architecture\n",
    "nn_arch = np.array([n_feature] + args.arch + [n_label])\n",
    "\n",
    "### Create the neural network\n",
    "W,B,act_func,nb_params = initNetwork(nn_arch,act_func_name)\n",
    "\n",
    "#############################\n",
    "### Optimization parameters\n",
    "#############################\n",
    "eta = args.eta\n",
    "batch_size = args.batch_size\n",
    "n_batch = int(math.ceil(float(n_training)/batch_size))\n",
    "n_epoch = args.n_epoch \n",
    "#regularization= args.regularization\n",
    "#my_lambda= args.my_lambda\n",
    "#############################\n",
    "### Auxiliary variables\n",
    "#############################\n",
    "cumul_time = 0.\n",
    "\n",
    "# Convert the labels to one-hot vector\n",
    "one_hot = np.zeros((n_label,n_training))\n",
    "one_hot[train_set[1],np.arange(n_training)]=1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Mini-batch creation\n",
    "j = 0\n",
    "batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "X_bacth = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape\n",
    "#len(batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appel de chaque fonction, avec les valeurs calculé avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Forward propagation\n",
    "\n",
    "Y,Yp = forward(act_func, W, B, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Y[0].shape == (n_feature, batch_size)\n",
    "assert Y[-1].shape == (10, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert Yp[0].shape == (nn_arch[1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Compute the softmax\n",
    "out = softmax(Y[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert out.shape == (nn_arch[-1], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute the gradient at the top layer\n",
    "derror = out-one_hot_batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "derror = out - one_hot_batch\n",
    "assert derror.shape == (nn_arch[-1], batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Backpropagation\n",
    "gradB = backward(derror, W, Yp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(gradB) == len(nn_arch)-1\n",
    "for gradw,dim in zip(gradB,nn_arch[1:]):\n",
    "    gradw.shape = (dim,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Update the parameters\n",
    "new_W, new_B = update(eta, batch_size, W, B, gradB, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Training accuracy\n",
    "train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "### Valid accuracy\n",
    "valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 821.375516 0.369016348983 0.91026 0.343456747075 0.9184 0.1\n",
      "1 909.396239 0.366707239643 0.91124 0.341364432125 0.9192 0.1\n",
      "2 998.864442 0.364367930994 0.91194 0.339254926853 0.9196 0.1\n",
      "3 1087.831657 0.362000013363 0.91246 0.337135096772 0.9206 0.1\n",
      "4 1186.368855 0.359628662761 0.91298 0.334998166515 0.9213 0.1\n",
      "5 1269.47664 0.357266862104 0.91356 0.332884369561 0.9219 0.1\n",
      "6 1353.78124 0.35497278009 0.91436 0.330825238291 0.9225 0.1\n",
      "7 1451.036884 0.352771320347 0.91516 0.328848779756 0.9232 0.1\n",
      "8 1561.876963 0.350685922929 0.91564 0.326970033909 0.9237 0.1\n",
      "9 1661.888579 0.348725106988 0.91622 0.325208963474 0.9249 0.1\n",
      "10 1759.277066 0.346893879939 0.91686 0.323567834487 0.9259 0.1\n",
      "11 1843.197111 0.345205418175 0.91724 0.322049419837 0.9259 0.1\n",
      "12 1927.136742 0.343646990679 0.91752 0.320647769895 0.9259 0.1\n",
      "13 2017.449707 0.342200973493 0.9179 0.319337898293 0.9259 0.1\n",
      "14 2100.150922 0.340881972593 0.91834 0.318135496896 0.9265 0.1\n",
      "15 2183.149256 0.339668274326 0.91892 0.317025138798 0.9268 0.1\n",
      "16 2274.854199 0.338557371426 0.91942 0.316004877767 0.9269 0.1\n",
      "17 2379.942162 0.337525236389 0.91974 0.315052958891 0.9274 0.1\n",
      "18 2491.648745 0.336574725437 0.91994 0.314174039764 0.9273 0.1\n",
      "19 2601.214809 0.335695916831 0.9203 0.313357771076 0.9278 0.1\n",
      "20 2698.256157 0.334882183312 0.92054 0.312602012378 0.9279 0.1\n",
      "21 2781.772227 0.334123600223 0.92088 0.311894695417 0.9279 0.1\n",
      "22 2884.282241 0.333418366038 0.92114 0.311235986163 0.9279 0.1\n",
      "23 2975.853874 0.332753146508 0.9213 0.31061889593 0.9282 0.1\n",
      "24 3086.40016 0.332137927204 0.92142 0.310040710648 0.9283 0.1\n",
      "25 3175.180127 0.331566768701 0.92162 0.309504396188 0.9284 0.1\n",
      "26 3284.57822 0.331029283792 0.92184 0.309006413941 0.9287 0.1\n",
      "27 3376.833654 0.330517349811 0.9219 0.308527180581 0.9291 0.1\n",
      "28 3461.882462 0.330035808319 0.92208 0.308077065396 0.9291 0.1\n",
      "29 3549.73238 0.329585346606 0.92232 0.307657503562 0.9296 0.1\n",
      "30 3649.845315 0.32916952454 0.92234 0.307271450475 0.9297 0.1\n",
      "31 3744.953073 0.328779651467 0.9224 0.306902406998 0.9298 0.1\n",
      "32 3828.969233 0.328414985327 0.92258 0.306560238332 0.93 0.1\n",
      "33 3911.341021 0.328067849558 0.92268 0.306232054928 0.9301 0.1\n",
      "34 4004.522397 0.327743745023 0.92272 0.305929272204 0.9302 0.1\n",
      "35 4093.079731 0.327432568859 0.92286 0.305635132306 0.9302 0.1\n",
      "36 4176.352811 0.327138092665 0.92288 0.305356647705 0.9303 0.1\n",
      "37 4276.176058 0.326858869997 0.92298 0.30509457914 0.9306 0.1\n",
      "38 4383.894779 0.326597582193 0.92326 0.304849231245 0.9306 0.1\n",
      "39 4486.998185 0.326351742963 0.92332 0.30461748502 0.9306 0.1\n",
      "40 4595.636984 0.326111361532 0.92334 0.304389182392 0.9307 0.1\n",
      "41 4693.11638 0.325883193779 0.92338 0.304173205871 0.9307 0.1\n",
      "42 4803.99471 0.325667173346 0.92326 0.303966708289 0.9309 0.1\n",
      "43 4912.469669 0.325469440335 0.92332 0.303775268903 0.931 0.1\n",
      "44 5027.036234 0.325273278492 0.92336 0.303590027119 0.9312 0.1\n",
      "45 5128.337329 0.325086888185 0.9234 0.303413358423 0.9313 0.1\n",
      "46 5227.252029 0.324910337362 0.9234 0.303243373872 0.9313 0.1\n",
      "47 5329.465715 0.324733538856 0.92348 0.303074215362 0.9314 0.1\n",
      "48 5457.389892 0.324570799154 0.92358 0.302916462986 0.9314 0.1\n",
      "49 5575.554611 0.324411241675 0.92364 0.302764749757 0.9314 0.1\n",
      "50 5692.338924 0.324256073161 0.92376 0.302617203004 0.9314 0.1\n",
      "51 5795.005691 0.324106568815 0.92384 0.302476225672 0.9314 0.1\n",
      "52 5882.743683 0.323966659563 0.92388 0.302341159544 0.9315 0.1\n",
      "53 5968.009412 0.323822675555 0.92386 0.302207814705 0.9316 0.1\n",
      "54 6050.726612 0.323687800798 0.92394 0.30207850062 0.9317 0.1\n",
      "55 6151.354512 0.323556306572 0.92402 0.301952060459 0.9317 0.1\n",
      "56 6239.735299 0.323439044003 0.92404 0.301841616643 0.9317 0.1\n",
      "57 6322.803679 0.323319026593 0.92416 0.30172658561 0.9317 0.1\n",
      "58 6413.085054 0.323208491073 0.92422 0.301620006175 0.9317 0.1\n",
      "59 6495.677883 0.32310078708 0.92426 0.301516692079 0.9318 0.1\n",
      "60 6583.702464 0.322996486593 0.92426 0.301419448792 0.9318 0.1\n",
      "61 6674.000744 0.322896046704 0.92432 0.301319973732 0.9318 0.1\n",
      "62 6777.412384 0.322796845711 0.92432 0.301227589332 0.9319 0.1\n",
      "63 6869.317625 0.322699455718 0.92438 0.301134494935 0.9319 0.1\n",
      "64 6960.412934 0.322604232695 0.9244 0.301043971933 0.932 0.1\n",
      "65 7049.159595 0.322521965134 0.92438 0.300964404156 0.932 0.1\n",
      "66 7140.583745 0.322436189356 0.92444 0.300882745563 0.932 0.1\n",
      "67 7231.73933 0.322351741125 0.92442 0.300801758953 0.9321 0.1\n",
      "68 7335.238769 0.322275309343 0.92446 0.30072904533 0.9321 0.1\n",
      "69 7434.402683 0.322203925599 0.92444 0.300661032821 0.9321 0.1\n",
      "70 7512.654736 0.322122957897 0.9245 0.300583521363 0.9321 0.1\n",
      "71 7595.306351 0.32204737668 0.92452 0.3005124656 0.9321 0.1\n",
      "72 7677.904792 0.321975678416 0.92454 0.300447834871 0.9321 0.1\n",
      "73 7760.996932 0.321903994998 0.92456 0.300382284948 0.9321 0.1\n",
      "74 7843.306579 0.321836971119 0.92458 0.300318160975 0.9321 0.1\n",
      "75 7936.392867 0.321779214869 0.92462 0.300264694545 0.9321 0.1\n",
      "76 8039.368385 0.321717061122 0.92462 0.300206364879 0.9321 0.1\n",
      "77 8136.091627 0.32165367345 0.92472 0.300147021793 0.932 0.1\n",
      "78 8236.800801 0.321595969586 0.9247 0.300092194172 0.932 0.1\n",
      "79 8357.35621 0.32154189764 0.92474 0.300041880891 0.932 0.1\n",
      "80 8455.868525 0.321489793218 0.92476 0.299993899024 0.932 0.1\n",
      "81 8565.647726 0.321437558085 0.92476 0.299945843173 0.9319 0.1\n",
      "82 8685.075414 0.321388726612 0.92478 0.299897219838 0.9319 0.1\n",
      "83 8772.941338 0.321340599928 0.92482 0.29985739629 0.9318 0.1\n",
      "84 8883.694778 0.321293533068 0.92484 0.299812341071 0.9318 0.1\n",
      "85 8972.410956 0.32124436376 0.92488 0.299766722143 0.9318 0.1\n",
      "86 9093.42199 0.321197233716 0.92488 0.299724428714 0.9318 0.1\n",
      "87 9201.983217 0.321148053976 0.92492 0.299676699231 0.9318 0.1\n",
      "88 9329.742524 0.321103336977 0.92492 0.299638215001 0.9318 0.1\n",
      "89 9454.100882 0.321058159645 0.9249 0.299594595568 0.9319 0.1\n",
      "90 9554.227392 0.321014540913 0.9249 0.299554051641 0.9319 0.1\n",
      "91 9652.929749 0.320970153786 0.92492 0.299514997057 0.9319 0.1\n",
      "92 9756.378731 0.32092970298 0.92494 0.299480320413 0.9319 0.1\n",
      "93 9846.443 0.320887388961 0.925 0.29944039831 0.9319 0.1\n",
      "94 9945.86826 0.320841990688 0.92494 0.299399597611 0.9319 0.1\n",
      "95 10060.89624 0.320804290293 0.92498 0.299365891485 0.9319 0.1\n",
      "96 10175.130343 0.32077040779 0.92504 0.299336079149 0.9319 0.1\n",
      "97 10291.733246 0.320734551457 0.92506 0.299306662319 0.9319 0.1\n",
      "98 10384.032995 0.320698661266 0.92504 0.299273384354 0.9318 0.1\n",
      "99 10490.490923 0.32066541844 0.92508 0.299243246866 0.9319 0.1\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "g_train_acc = []\n",
    "g_valid_acc = []\n",
    "for i in range(n_epoch):\n",
    "    \n",
    "    for j in range(n_batch):\n",
    "\n",
    "        ### Mini-batch creation\n",
    "        batch, one_hot_batch, mini_batch_size = getMiniBatch(j, batch_size, train_set, one_hot)\n",
    "\n",
    "        prev_time = time.clock()\n",
    "\n",
    "        ### Forward propagation\n",
    "        Y,Yp = forward(act_func, W, B, batch)\n",
    "\n",
    "        ### Compute the softmax\n",
    "        out = softmax(Y[-1])\n",
    "        \n",
    "        ### Compute the gradient at the top layer\n",
    "        derror = out-one_hot_batch\n",
    "\n",
    "        ### Backpropagation\n",
    "        gradB = backward(derror, W, Yp)\n",
    "\n",
    "        ### Update the parameters\n",
    "        W, B = update(eta, batch_size, W, B, gradB, Y)\n",
    "\n",
    "        curr_time = time.clock()\n",
    "        cumul_time += curr_time - prev_time\n",
    "\n",
    "    ### Training accuracy\n",
    "    train_loss, train_accuracy = computeLoss(W, B, train_set[0], train_set[1], act_func) \n",
    "\n",
    "    ### Valid accuracy\n",
    "    valid_loss, valid_accuracy = computeLoss(W, B, valid_set[0], valid_set[1], act_func) \n",
    "\n",
    "    result_line = str(i) + \" \" + str(cumul_time) + \" \" + str(train_loss) + \" \" + str(train_accuracy) + \" \" + str(valid_loss) + \" \" + str(valid_accuracy) + \" \" + str(eta)\n",
    "\n",
    "    #g_i = np.append(g_i, i)\n",
    "    #g_train_loss = np.append(g_train_loss, train_loss)\n",
    "    g_train_acc = np.append(g_train_acc, train_accuracy)\n",
    "    #g_valid_loss = np.append(g_valid_loss, valid_loss)\n",
    "    g_valid_acc = np.append(g_valid_acc, valid_accuracy)\n",
    "    print(result_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae4d9534d0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGHCAYAAACNjTnqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+f+uqF+1ciAc1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae4d98bc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_train_acc,label='train_acc')\n",
    "plt.plot(g_valid_acc,label='valid_acc')\n",
    "plt.title(\"accuracy RELU l2 0.01\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accurancy\")\n",
    "plt.ylim([0.,1.])\n",
    "plt.legend(loc = \"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
